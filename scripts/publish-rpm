#!/bin/bash
# Creates a local RPM repo, then pushes its contents to S3, for serving.
set -e
set -u
set -o pipefail


REPO_ROOT="$(git rev-parse --show-toplevel)"
RPM_LOCAL_DIR="${REPO_ROOT}/rpm-repo"


function list_rpms() {
    find "$RPM_LOCAL_DIR" | grep '\.rpm$'
}

function container_run() {
    docker run --rm \
        --network=none \
        -v "$RPM_LOCAL_DIR:/repo" \
        fpf.local/createrepo \
        $@
}

# Check that we have local RPMs to build a repo for
if [[ -z "$(list_rpms)" ]]; then
    echo "No RPM files found in $RPM_LOCAL_DIR"
    echo "Build RPMs and place in that directory, then rerun the publish action."
    exit 1
fi

# Ensure we have 'aws' installed, otherwise we cannot push to S3
if ! hash aws > /dev/null 2>&1 ; then
    echo "'aws' CLI not found, install requirements"
    exit 2
fi

# Build container for preparing repo
docker build -t fpf.local/createrepo -f docker/CreateRepoRPM/Dockerfile .

# TODO: In order to manage state over time, we'll need to *pull* from S3,
# populating the existing local dir with the current state of what's in S3.
# That's a bandwidth-intensive operation, so skipping for now.
# aws --profile sdpackager s3 sync "s3://dev-bin.ops.securedrop.org/dom0-rpm-repo/ ${RPM_LOCAL_DIR}/"

# Sanity check that we have RPMs locally to upload, and they're already
# signed.
echo "Validating RPM signatures..."
while read -r f; do
    fname="$(basename "$f")"
    sig_results="$(container_run rpm -Kv "$fname")"
    if ! grep -qP '^\s+V4 RSA/SHA(256|512) Signature, key ID \w+: OK$' <<< "$sig_results"; then
        echo "Failed to validate signature on $fname"
        echo "Is the RPM signed? rpm -Kv showed:"
        echo "$sig_results"
        exit 3
    fi
done <<< "$(list_rpms)"

# Use local container for creating repo metadata
container_run createrepo_c .

# Push created repo dirtree to S3
aws --profile sdpackager s3 sync \
    --exclude ".empty" \
    "${RPM_LOCAL_DIR}/" s3://dev-bin.ops.securedrop.org/dom0-rpm-repo/
